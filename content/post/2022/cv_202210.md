---
title: "简历解析"
date: 2022-10-19T16:01:23+08:00
lastmod: 2022-10-19T16:01:23+08:00
draft: true
tags: ["简历", "详细准备"]
categories: ["自省", "思悟"]
author: "clavenzhang"

weight: 1

# You can also close(false) or open(true) something for this content.
# P.S. comment can only be closed
# comment: false
# toc: true
mathjax: true
---

## 前言
简历内容详细准备，一些重点要发力的内容。
一个聚焦点：TStack从无到有的发展路线，简历其他内容稍微提一下就行，主要是要有完整的工作经历。
先讲云平台的发展历程：
c#阶段，以基础开源IAAS底座(openstack)作为基础，在上面构建以大单体服务为主的全栈私有云平台。当时最新也是主流的管理平台框架都是MVC模式的，前后端分层，controller在中间过渡，负责处理数据封装和交互逻辑。后台部分，则以服务职责为界限，划分了权限管理的RBAC系统、数据和资源仓库CMDB、基础流程引擎以及负责屏蔽PAAS、SAAS层异构逻辑的云服务适配器等基础服务，这些是云管平台的基础服务。然后，在基础服务的上面，构造了三个大的平台，分别是自助管理平台、监控平台以及运维管理平台。自助管理平台，负责的是对计算、存储、网络、PaaS接入，还有中间件的各项能力提供业务逻辑封装和数据管理能力，比如虚拟机、云存储、弹性IP、TDSql、存储网关等。
监控平台，当时最重要的是负责监控物理宿主机和生产的虚拟机包括cpu、内存、磁盘等各项指标的健康状态，实时监测云平台的运行状态。
运维管理平台，主要提供资产管理、服务流程管理、服务计费以及脚本下发的一些功能。
后台的这些服务，其实不算严格意义上的单体服务，服务还是按照职责做了一定的拆分，对于故障定位、快速恢复还是有一定的帮助。
开发团队依托svn管理代码以及协作开发，每个私有化部署的环境维护一套分支代码。这种模式，在初期快速发展的阶段，好处是：门槛和管理成本低，业务可以快速向前滚。

单体应用，开发速度变慢(一个小的改动，要全部构建，且要保证不影响其他模块)，测试难度变大(要覆盖所有内容)，部署周期变长(多个服务共用数据库，大概率有数据耦合，一次升级部署需要做到数据库脚本兼容所有的业务，并确保可以回滚)。没有故障隔离，单体服务的一个模块故障，可能会引发整个全局的业务不可用。难以对单个模块进行特别的扩展：比如一些重要的模块需要消耗的资源高，没有办法单独给它分配相应的资源。技术栈升级更新难度高。

我们在14年起步开始做云，到16年底的时候，云平台基本上已经集成了云的大部分能力。随着交付的客户越来越多，我们发现，平台研发、交付和运维的压力越来越大。我记得当时做版本变更的时候，开发、测试、变更的20几号人在周末连续通宵来实现平台升级。这个时候，系统和理念较当时业界最先进的技术方案已经落后了。所以我们也就来到了云平台的第二个阶段，分布式微服务化。
我们在不断集成和发展云平台能力的同时，也是时刻关注着云最新技术的动向，微服务可以说是云时代最重要的技术手段之一。
![micro](/blog/2022/micro-service.jpg)

优点：
大型复杂的项目可以持续交付部署
每个服务相对小且易于维护
独立部署
独立扩展
团队独立自治
更容易实验和采纳新技术
容错性(单个服务的问题不影响其他服务)

服务注册和发现，两种模式：客户端模式和服务端模式
客户端模式：服务注册到注册中心，其他服务需要访问该服务的时候，从注册中心获取服务实例列表，然后在本地通过负载均衡策略决定访问哪个实例。
服务端模式：服务实例同样注册到注册中心，在客户端和服务端中间引入了一个router角色，router负责从注册中心拿实例列表，然后再通过负载均衡策略选择实例。

第一种方式，在访问的路由上，少了一跳。缺点是，对于服务有侵入性。但同时，每个服务可以指定自己对于目标实例的负载均衡策略。
第二种方式，网络上路由链路多一跳，对于服务无入侵。
两种方式各有优劣，我们实际实现的是，服务发现负载均衡模块抽取成公共组件，最小化对于业务层的入侵，提供不同的负载均衡策略(轮询负载均衡/加权平均/哈希映射/最快时间响应优先等)，服务只需要配置即可。


k8s的服务是对一系列pod以及访问他们的策略的逻辑抽象。k8s apiserver就是服务注册中心。kube proxy其实就是对应服务端服务发现模式下的router角色，负责路由和流量均衡转发。kube proxy后面对接的endpoint就是注册到apiserver的具体业务pod。

k8s ingress，作为集群内部服务的入口，将内部http流量暴露给外部使用，相当于一个简化版的api gateway.
真正的api gateway-k8s Gateway Api.
声明式api，对比的是命令式api，即我们常用的cli，比如创建两副本的nginx： docker service create --name nginx --replicas 2 nginx
k8s首先需要一个yaml文件，然后使用kubectl apply去进行相关操作。声明式api只需要你定义你要系统做的事情，然后系统根据定义处理你定义的内容。

不可变基础设施(immutable frastructure)，docker和k8s信奉的理念，都是基础设施是不可变的。比如pod，更新pod一定是创建一个新的，然后删除旧的。基础的架构不能受影响。

云原生的核心：应用弹性可伸缩。根据cpu和内存的使用率，做hpa-pod水平自动伸缩(HorizontalPodAutoscaler)，弹性伸缩。定时弹性伸缩，有的服务，负载是有规律的，就可以做定时策略。
Pod 水平自动扩缩控制器根据当前指标和期望指标来计算扩缩比例。
期望副本数 = ceil[当前副本数 * (当前指标 / 期望指标)]

服务熔断和服务降级，这是两个概念。服务熔断，是主动进行的，当在一定的时间窗口内，调用目标服务的失败次数达到一定数量，则开启主动熔断，执行本地的默认方法，不再进行网络请求。熔断-半熔断-熔断恢复，进入熔断状态之后，过一段时间，尝试恢复服务调用，允许有限的流量调用该服务，并监控调用的成功率，当成功率达到预期，则表示服务已经恢复，进入熔断恢复状态。如果成功率依旧很低，则重新进入熔断状态。熔断是框架层的，只要在接口层实现即可。
服务降级，则是服务级别的，服务降级有多种方式：开关降级/限流降级/熔断降级。
服务降级，首先需要埋点，梳理哪些是核心功能，哪些是非核心功能，在非核心功能上加上开关，需要降级的时候，把非核心功能关闭。
降低一致性，将核心业务的同步逻辑改成异步，强一致性变成最终一致性。
服务降级是在具体服务层的，需要预先做好降级策略，并写入到代码结构中。

框架层实现微服务所需要的服务注册、服务发现、流量管理(熔断和降级)、api网关的内容，微服务开发者只需要关注业务本身的逻辑即可。
服务网关是单独的进程，可以独立维护，服务只需要配置相关内容就行。
入流量-ingress  出流量-egress

随着业务的不断发展，单体应用能够承载的容量将逐渐到达上限，即使通过应用改造来突破垂直扩展（Scale Up）的瓶颈，并将其转化为支撑水平扩展（Scale Out）的能力，在全局并发访问的情况下，也依然会面临数据计算复杂度和存储容量的问题。因此，需要将单体应用进一步拆分，按业务边界重新划分成分布式应用，使应用与应用之间不再直接共享数据，而是通过约定好的契约进行通信，以提高扩展性。而服务数量的增加必然导致运维的成本增加，因此微服务化、容器化和环境治理成为了服务化能力建设的三个重点。

云原生（CloudNative）是一个组合词，“云”表示应用程序运行于分布式云环境中，“原生”表示应用程序在设计之初就充分考虑到了云平台的弹性，就是为云设计的。可见，云原生并不是简单地使用云平台运行现有的应用程序，而是一种能充分利用云计算优势对应用程序进行设计、实现、部署、交付和操作的应用架构方法。

配置变更热加载，配置动静态分离，不可修改的(端口/日志配置等)，可修改的(其他业务类配置)。静态配置在第一次渲染的时候打入镜像，实时监听配置中心动态配置变更，主动热加载，无需重启服务。

弹性伸缩，自动扩缩容，策略：快扩容，慢缩容。应对突发/降低成本/保持稳定。
流量突增，自动扩容。日常运营：告警驱动人工扩容。上下游同步扩缩容。

弹性扩缩容根据我们的实践，需要遵循的一个原则就是“快扩容、慢缩容”。
快扩容很好理解，在需要扩容的时候我们希望尽快的扩容出来Pod来承担增长的流量。
那怎么来理解慢缩容？慢缩容主要考虑：
- CPU使用率短时间波动比较大，如果很快缩容后，又需要再扩容出来。
- 缩容速度如果过快，很有可能导致缩容后利用率上升又需要扩容。
对于生产环境不宜频繁的弹性扩缩容，需要仔细的配置容忍度和利用率阈值，让扩缩容在合理的范围内进行。

服务平滑升级和无损发布。

日志平台的logstash消费算法，logstash是系统的瓶颈，要解决该瓶颈问题。首先，使用消息队列做数据缓冲，启动监控服务，监测消息队列数据积压情况，根据积压情况动态调整logstash消费实例。

云原生的关键要素：DevOps/CICD/微服务/容器/服务网格


监控运维一体化

关于CI，CI不仅仅是研发的范畴，频繁的发布更新占用了PM、开发、测试大量时间，如果能为各个角色快速定制一个专属的工作流，联动平台、推进流程、关联到人，可视化全程进展，以任务来驱动多团队协同，能提高效率节省大量沟通时间。

google的服务，每隔一段时间，就要重写。注意，是重写，不是重构。没有历史包袱，没有技术债压力，针对已经踩过的坑，结合现有最新的技术，重写服务。

水平伸缩：增加副本数量  垂直伸缩：加大配置

应用容器化：dockerfile文件定义了运行时环境和依赖，可以跨平台运行。
动态编排调度：k8s 如果没有编排调度，运维每天的工作就是到处去跑docker run，挂数据卷、开端口、管理网络等等，而k8s就是自动化管理这一切的自动化调度编排工具。
面向微服务：应用灵活性和可扩展性，符合云原生敏捷迭代快速试错的基本原则。
应用微服务化：DDD(领域驱动设计)微服务拆分。

分布式配置中心：
- 配置和代码分离，环境隔离
- 语言无关，框架无关
- 中心化管理，热更新
- 本地文件备份，容灾


docker image是分层的镜像组成，比如它需要打包轻量的linux操作系统，然后在上面有其他的镜像层，最上面是应用镜像。docker pull的时候，如果有相同的层，不会重复下载，比如同一个应用的不同版本的镜像，pull的时候，后面的会快很多，是因为有很多重复的layer

docker和vm的区别
操作系统分为两层，一层是内核(linux core)，一层是应用层(常说的用户态)
docker打包的操作系统，其实是操作系统的应用层，它没有内核，与宿主机共用内核，所以docker image才有x86、arm以及其他类型的区别。
而vm则不一样，vm的操作系统有自己的内核和应用层，它把整个操作系统都做了虚拟化。虚拟机可以在任何操作系统上进行，是更复杂也更笨重的一种方式。
所以docker的镜像跟vm的镜像有着量级的差别，docker镜像一般都是MB，而vm都是GB.


容器相对于传统应用的优势：
- 举个例子，一共有ABCDEFG7个服务组成的系统，它们原本用的都是同样的数据库，比如mysql5.6，但是有一天A服务发现自己要实现的业务逻辑里面依赖的某个东西，必须要用到高版本的mysql，比如至少5.7以上，才能支持。这个时候，我们只能把所有的服务都切换成5.7，不然A服务的这个功能就上不了。但是，这样做是有极大风险的，我们首先得把历史功能全面回归，确保升级不会有问题。同时，在部署落地的场景中，需要在客户现场升级数据库，又是一个新的风险点。如果是用容器的话，则完全不会有任何问题，A服务需要5.7的版本，就部署两个不同版本的容器，A服务用5.7，其他服务用5.6。甚至，我们可以7个服务用7个版本都没有问题，只要有需要。